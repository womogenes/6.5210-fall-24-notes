## Fibonacci heaps
A Fibonacci heap is a collection of heap-ordered trees with a pointer to the min element
- On insert, we are lazy (add a tree & possibly update the min pointer)
- On delete-min, we need to find the next min and then consolidate the HOTs into a smaller number of HOTs
	- Consolidation is linear in the number of roots â€” it's always (# of roots - 1)
	- Performance opportunities exist in **which comparisons happen**
	- Union by rank: only compare heap-ordered trees of the same rank. Then a degree-k tree becomes a degree-k+1 tree, and thus has $2^k$ nodes [**WHY??**]
	- The degree of a node is helpful b/c it tells us how many trees get added when we remove this node

Implementation:
- Every node has four pointers (parent, first child, next sibling, previous sibling)

Notes:
- We want to preserve the property that every tree whose root has $k$ children has $2^k$ nodes

### Analysis
- Potential function: $\Phi(DS) \rightarrow \mathbb R$
- The amortized cost of an operation is the actual time + $\Phi(\text{end}) - \Phi(\text{start})$.
- Amortized cost is greater than the real cost so long as the final potential is greater than the starting potential
- We define the potential function "very naturally" to be the # of heap-ordered trees. This aligns with the idea of potential as **the amount of work we've put off till later**

**Add operation**
- Cost of **insertion** is O(1), change in potential is +1, amortized cost is still O(1)
- Cost of **delete-min** for a $c$-child root among $r$ HOTs:
	- $c$ steps to insert the children into the HOT list
	- $r + c + \log n$ steps to consolidate the roots
	- Initial $\Phi$ is $r$, the final $\Phi$ is at most $\log n$ because there is at most one heap-ordered tree per bucket (where each bucket consists of the trees of given degree) and there are at most $\log n$ buckets
	- Thus the amortized cost is $(r + c) + (\log n) - r = c + \log n$ which is on the order of $\log n$ since $c$ is at most $\log n$ (can't allow any node to have too many children)
- Cost of **merge**: O(1) because the HOTs are stored as two linked lists, we can add them together
	- Potential change is 0, so amortized cost is O(1)
- Cost of **decrease-key**: assume we already have a pointer to the node
	- We can do this in O(1) real time by cutting off the child and sending it back to the list of HOTs. Potential change is also O(1).
	- **However,** when a node loses a second child, we cut *it* from its parent + make it a root.
	- We keep track of how many children a node has lost by maintaining a **marked** bit

**Claim:** cascading cuts are actually free
- i.e. ensure the potential function pays for this work
- Make the potential function *also* involve the number of marked bits
- We need the potential function to measure the amount of "deferred work" in the tree
- Every cascade cut clears the **marked** bit of each node it cuts (thus is paid for)

**Lemma:** the $i$th child of $x$ that is still remaining has degree at least $i - 2$.
- Recall: we only add a node as a child of another when the two nodes have the same degree
- Proof: Let $y$ be the $i$th added child. Then when $y$ became a child of $x$, both had at least $i-1$ children.
- At this point, $y$ has lost at most 1 child, so it has at least $i-2$ children.

Let $S_k$ be the minimum possible number of descendants (including self) of a degree $k$ node.
- We can write a recurrence.
- Start with $S_0 = 1$.
- $S_1 = 2$ because it has degree 1 and thus has a child.
- $S_2 = 3$
	- Self and two children.
	- One child was added first, so has degree at least -1 (has at least self)
	- Second child was added second, so has degree at least 0
- $S_3 = 5$
	- First node has at least self.
	- Second node has at least self and one child.
	- Third node has at least 2 descendants (including itself)

Bringing it together:
$$S_k = 2 + \sum_{i=0}^{k-2} S_i. $$
This simplifies to $S_k = S_{k-1} + S_{k-2}$.

Corollary: The maximum degree of a $k$-node tree is $\log_{\phi} k = O(\log k)$.
## Coda no. 1: speed in practice
- Fibonacci heaps sometimes won over regular binary heaps on small devices (wall-clock time)
- We should be skeptical about Fibonacci heaps though
	- Binary heaps in arrays ("implicit heaps") don't need pointer manipulation because the indices serve as implicit pointers
	- Pointers are bad in modern computers because you get bad "locality of reference"
## Coda no. 2: minimum spanning tree
- Prim's algorithm ($n$ nodes, $m$ edges)
	- $n$ inserts
	- $n$ deletes
	- $m$ decrease-keys
- Total complexity: $O(m + n \log n)$ time. This is essentially linear in number of edges except for the sparsest graphs.
- The deletes are the bottleneck (each costs $\log n$ time). This is expensive because there are $n$ items in the heap. If we keep the heap small, we're golden.
	- Idea: cap the heap size at $k$. As long as this is true, run Prim's algorithm normally, and we're running at $O(k \log k)$.
	- When you access a node, you look at its neighbors, and put them in the heap if they're not in yet
	- What happens when the heap grows past size $k$?
		- Go somewhere else and start growing a new tree until we hit a tree that we've made previously.
		- We now have a forest of trees, each with at most $k$ neighbors
		- To make progress, we draw a circle around each tree and basically make a new graph with each tree as a node and edges between the nodes. Essentially create new nodes, one for each circle, and give each node the edges that belonged to the circle.
		- **Recurse!** but choose a new $k$

What is the schedule of $k$s?
- Phase that starts with $t$ vertices (which we got by contracting the forest of the previous phase)
- Pick a region size $k$ and grow trees until all trees have $k$ incident edges
- Since the heap size is at most $k$, the runtime is $m + (t \log k)$.
- We might as well have $t \log k = m$ because that doesn't change asymptotic runtime
- The runtime of a phase is therefore just $O(m)$.
How many phases are there?
- Each contracted vertex has $k$ edge endpoints (because the original circle it was contracted from had $k$ neighbors)
- There are only $m$ endpoints total ($2m$?)
- There are at most $m/k$ vertices then
- The next phase has $t' = 2^{m/t'}$ vertices. We know $m/t' = 2^k$ ... so this turns into a power tower
- The number of phases is thus equal to the height of a power tower of 2s that equals of $n$.
This is pretty small, but "theoreticians are never satisfied"

**Improvements**
- Later, Tarjan & some other people improved this to $\log \beta(m, n)$ time.
- Chazelle improve this to $m \alpha(n) \log \alpha(n)$ where $\alpha$ is the *inverse Ackermann function*
- Pettie & Ramachandran devised a better, optimal algorithm
	- https://web.eecs.umich.edu/~pettie/papers/jacm-optmsf.pdf
	- We don't know the runtime of this algorithm :/
	- We do know that you can solve MST by linear time in $O(n)$ comparisons
	- There exists a *randomized* algorithm
## Questions
- How do you limit the number of children per node
- Alex: "why are these called Fibonacci heaps?"